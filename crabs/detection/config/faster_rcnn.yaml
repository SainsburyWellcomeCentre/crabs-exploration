# Dataset
#-------------
train_fraction: 0.8
val_over_test_fraction: 0.5
num_workers: 4

# -------------------
# Model architecture
# -------------------
num_classes: 2

# -------------------------------
# Training & validation parameters
# -------------------------------
n_epochs: 250
learning_rate: 0.00005
wdecay: 0.00005
batch_size_train: 4
batch_size_val: 4
checkpoint_saving:
  every_n_epochs: 50
  keep_last_n_ckpts: 5
  save_weights_only: True
  copy_as_mlflow_artifacts: True
  # copy_as_mlflow_artifacts:
  # if True, selected checkpoints are added as artifacts at the end of training,
  # if all, all checkpoints for every epoch are added as artifacts during training,
  # if False, no checkpoints are added as artifacts.
  save_last: True

# -----------------------
# Evaluation parameters
# -----------------------
iou_threshold: 0.1
batch_size_test: 4

# -------------------
# Data augmentation
# -------------------
gaussian_blur:
  kernel_size:
    - 5
    - 9
  sigma:
    - 0.1
    - 5.0
color_jitter:
  brightness: 0.5
  hue: 0.3
random_horizontal_flip:
  p: 0.5
random_rotation:
  degrees: [-10.0, 10.0]
random_adjust_sharpness:
  p: 0.5
  sharpness_factor: 0.5
random_autocontrast:
  p: 0.5
random_equalize:
  p: 0.5
clamp_and_sanitize_bboxes:
  min_size: 1.0
# ----------------------------
# Hyperparameter optimisation
# -----------------------------
# when we run Optuna, the n_trials and n_epochs above will be overwritten by the parameters set by Optuna
optuna:
  # Parameters for hyperparameter optimisation with Optuna:
  # - We can optimize `learning_rate`, `num_epochs` or both
  # - n_trials defines the total number of trials ran in the optimisation
  n_trials: 3
  # The lower bound and the upper bound of the learning rate parameter
  learning_rate:
    - 1e-6
    - 1e-4
  # The lower bound and the upper bound of the number of epochs
  n_epochs:
    - 1
    - 3
